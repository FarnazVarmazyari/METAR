# Databricks notebook source
# MAGIC %md 
# MAGIC 
# MAGIC # METAR - Model Fitting and Evaluation 
# MAGIC 
# MAGIC In this file, we fit models and evaluate them on the preprocessed data. In order to have the data ready for modeling, we run the `5_Data_Preprocessing_Code` Notebook from here using the following `%run`command.
# MAGIC 
# MAGIC This command is roughly equivalent to an import statement in python. All variables defined in the other notebook become available in the current notebook.

# COMMAND ----------

# MAGIC %run ./5_Data_Preprocessing_Code

# COMMAND ----------

# MAGIC %md 
# MAGIC 
# MAGIC The goal of this study is to predict if there is any precipitation(regardless of its type) given all the information about wind, visibility, clouds, temperature and pressure at the moment that the METAR has been captured. This is an example of one of the most common classification problems, a binary classification.
# MAGIC 
# MAGIC In this notebook, we will fit 3 common classification models on our data and compare their accuracy through `areaUnderROC`. The other available option to compare the models is `areaUnderPR()` which computes the area under the precision-recall curve. `spark.mllib` supports two linear methods for classification: linear Support Vector Machines (SVMs) and Logistic Regression. Linear SVMs supports only binary classification, while logistic regression supports both binary and multiclass classification problems. For both methods, `spark.mllib` supports L1 and L2 regularized variants. We start with the latter linear classification method which is logistic regression.

# COMMAND ----------

# MAGIC %md Importing all the required libraries:

# COMMAND ----------

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.classification import RandomForestClassifier

# COMMAND ----------

# MAGIC %md 
# MAGIC # Pipeline
# MAGIC 
# MAGIC Before moving forward with modeling, first we create a pipeline.
# MAGIC 
# MAGIC In machine learning, it is common to run a sequence of algorithms to process and learn from data. In our case, we have doen most of processing before so the remaining steps consists of the following:
# MAGIC 
# MAGIC - Converting rows in the dataframe into a numerical feature vectors and labels.
# MAGIC - Learning a prediction model using the feature vectors and labels.
# MAGIC 
# MAGIC `MLlib` represents such a workflow as a Pipeline, which consists of a sequence of `PipelineStages` (Transformers and Estimators) to be run in a specific order. 
# MAGIC 
# MAGIC # How it works
# MAGIC 
# MAGIC A Pipeline is defined as a sequence of stages. Each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. In Transformer stages, the `transform()` method is called on the DataFrame. For Estimator stages, the `fit()` method is called.
# MAGIC 
# MAGIC The following are the stages of our pipeline:
# MAGIC - Vector Assembler
# MAGIC - String Indexer

# COMMAND ----------

# MAGIC %md ### Vector Assembler
# MAGIC 
# MAGIC VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. 
# MAGIC VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.
# MAGIC 
# MAGIC In the process, we will ignore target as well as columns that are not estimators.

# COMMAND ----------

stages = [] # stages in our Pipeline

ignore = ['station', 'valid', 'lon', 'lat', 'NotAvailable', 'precipitation']
assembler = VectorAssembler(inputCols=[x for x in new11.columns if x not in ignore],outputCol='features')

stages += [assembler]

# COMMAND ----------

# MAGIC %md ### String Indexer
# MAGIC 
# MAGIC We use the `StringIndexer()` to encode our labels to label indices. In our case the target is already a binary variable. To make it more scaleble for future modeling efforts and constructing a general pipeline, we used this method. In reality no transformation is happening which means `precipitation` and `label` are the same. 

# COMMAND ----------

label_stringIdx = StringIndexer(inputCol = "precipitation", outputCol = "label")

stages += [label_stringIdx]

# COMMAND ----------

# MAGIC %md
# MAGIC Next, we run our stages as a Pipeline. This puts the data through all of the feature transformations we described in a single call. The `fit()` method computes feature statistics as needed. The `transform()` method actually transforms the features. At this point, we can disregard original columns and only keep relevent columns for modeling.

# COMMAND ----------

# Create a Pipeline.

pipeline = Pipeline(stages=stages)
# Run the feature transformations.

pipelineModel = pipeline.fit(new11)
data = pipelineModel.transform(new11)
# Keep relevant columns

selectedcols = ["label", "features", "station", "valid"] 
data = data.select(selectedcols)
display(data)

# COMMAND ----------

# MAGIC %md # Training and test sets

# COMMAND ----------

# MAGIC %md 
# MAGIC Finally we create the training and test sets through `randomSplit`. We set seed to 1 for reproducibility.

# COMMAND ----------

(trainingData, testData) = data.randomSplit([0.8, 0.2], seed = 1)

print trainingData.count()
print testData.count()

# COMMAND ----------

# MAGIC %md The training and test DataFrames have 2,381,815 and 595,100 observations respectively. 

# COMMAND ----------

# MAGIC %md # Logistic Regression

# COMMAND ----------

# MAGIC %md After understanding the data and preparing it for modeling, we can now try out some of the binary classification algorithms.
# MAGIC 
# MAGIC The general steps of modeling: 
# MAGIC 
# MAGIC - Creating initial model using the training set 
# MAGIC - Tuning parameters with a ParamGrid and 3-fold Cross Validation 
# MAGIC - Evaluating the best model obtained from the Cross Validation using the test set
# MAGIC 
# MAGIC The `BinaryClassificationEvaluator` is used to evaluate our models. The default metric is `areaUnderROC`.

# COMMAND ----------

# MAGIC %md ### Initial model:

# COMMAND ----------

# MAGIC %md First, The initial LogisticRegression() model is created and the training data is used to train the model.

# COMMAND ----------

lr = LogisticRegression(labelCol="label", featuresCol="features", maxIter=10)

lrModel = lr.fit(trainingData)

# COMMAND ----------

# MAGIC %md
# MAGIC Next, we make a prediction on the test data through `transform()` method. `LogisticRegression.transform()` will only use the `features` column.

# COMMAND ----------

predictions = lrModel.transform(testData)

# COMMAND ----------

# MAGIC %md As a result of `transform()` method 3 new column, `rawPrediction`, `probability` and `prediction` are added to the original dataframe to create the `predictions` dataframe.
# MAGIC 
# MAGIC `predictions` dataframe has following schema:

# COMMAND ----------

predictions.printSchema()

# COMMAND ----------

# MAGIC %md Take a look at  some of the model's predicttion and corresponding probabilities of each class: 

# COMMAND ----------

predictions.select("label", "prediction", "probability").collect()


# COMMAND ----------

# MAGIC %md The `BinaryClassificationEvaluator` method is used to evaluate our model. 
# MAGIC 
# MAGIC The Evaluator expects two input columns: 
# MAGIC - `rawPrediction` 
# MAGIC - `label`

# COMMAND ----------

# Evaluate model
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
evaluator.evaluate(predictions)

# COMMAND ----------

# MAGIC %md ### Tuning the model parameters

# COMMAND ----------

# MAGIC %md Next, we tune the model with the `ParamGridBuilder` and the `CrossValidator`.
# MAGIC Available params for tuning, can be found and explored through `explainParams()`. 

# COMMAND ----------

#ParamGrid for Cross Validation
paramGrid = (ParamGridBuilder()
             .addGrid(lr.regParam, [0.01, 0.5, 2.0])
             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])
             .addGrid(lr.maxIter, [1, 5, 10])
             .build())

# COMMAND ----------

# MAGIC %md  Since we assigned 3 values for `regParam` which is regularization parameter, 3 values for `maxIter` or max number of iterations, and 3 values for `elasticNetParam` which is the the ElasticNet mixing parameter, this grid will have 27 parameter settings for CrossValidator to choose from. 
# MAGIC 
# MAGIC A 3-fold cross validator is used.

# COMMAND ----------

cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)

cvModel = cv.fit(trainingData)
# this code runs for a long time!

# COMMAND ----------

# MAGIC %md ### Evaluating the best model

# COMMAND ----------

# MAGIC %md Finally we use test data to to measure accuracy of our tuned model.

# COMMAND ----------

predictions = cvModel.transform(testData)
evaluator.evaluate(predictions)

# COMMAND ----------

# MAGIC %md
# MAGIC # Decision Tree
# MAGIC 
# MAGIC Some of the reasons to use decision trees are:
# MAGIC 
# MAGIC - They are easy to interpret.
# MAGIC - They can handle categorical features. 
# MAGIC - They extend to the multiclass classification setting.
# MAGIC - They do not require feature scaling. 
# MAGIC - They also are able to capture non-linearities and feature interactions. 
# MAGIC 
# MAGIC The implementation of decision tree in spark partitions data by rows, allowing distributed training with millions of instances.

# COMMAND ----------

# MAGIC %md ### Intial Model

# COMMAND ----------

# MAGIC %md First, we create the initial moddel and train the model with the training data.

# COMMAND ----------

dt = DecisionTreeClassifier(labelCol="label", featuresCol="features", maxDepth=3)

dtModel = dt.fit(trainingData)

# COMMAND ----------

# MAGIC %md We extract the number of nodes in the decision tree as well as the tree depth as follow:

# COMMAND ----------

print "numNodes = ", dtModel.numNodes
print "depth = ", dtModel.depth

# COMMAND ----------

# MAGIC %md In the next step, we make prediction on the test data through `transform()` method. 

# COMMAND ----------

predictions = dtModel.transform(testData)

# COMMAND ----------

# MAGIC %md Take a quick look at schema of the resulting dataframe:

# COMMAND ----------

predictions.printSchema()

# COMMAND ----------

# MAGIC %md Model's predictions and probabilities for each prediction class for every observation is acquired as follow:

# COMMAND ----------

predictions.select("label", "prediction", "probability").collect()

# COMMAND ----------

# MAGIC %md Next,we evaluate our Decision Tree model with `BinaryClassificationEvaluator`.

# COMMAND ----------

evaluator = BinaryClassificationEvaluator()
evaluator.evaluate(predictions)

# COMMAND ----------

# MAGIC %md %md ### Tuning the model parameters

# COMMAND ----------

# MAGIC %md We are ready to tune the model with the `ParamGridBuilder` and the `CrossValidator`.
# MAGIC 
# MAGIC We are defining 4 values for `maxDepth` which is Maximum depth of the tree and 3 values for `maxBin`or Max number of bins for discretizing continuous features, the grid will have 12 parameter settings for CrossValidator to choose from. 
# MAGIC 
# MAGIC A 3-fold CrossValidator is created.

# COMMAND ----------

paramGrid = (ParamGridBuilder()
             .addGrid(dt.maxDepth, [1,2,6,10])
             .addGrid(dt.maxBins, [20,40,80])
             .build())

# COMMAND ----------

# MAGIC %md Next step is to create the 3-fold `CrossValidator` and to fit the training data.

# COMMAND ----------

cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)

cvModel = cv.fit(trainingData)
# Takes good amount of time!

# COMMAND ----------

# MAGIC %md Take a look at best model's characteristics:

# COMMAND ----------

print "numNodes = ", cvModel.bestModel.numNodes
print "depth = ", cvModel.bestModel.depth


# COMMAND ----------

# MAGIC %md ### Evaluating the best model

# COMMAND ----------

# MAGIC %md We use test set here so we can measure the accuracy of the model from grid on new data.

# COMMAND ----------

predictions = cvModel.transform(testData)
evaluator.evaluate(predictions)

# COMMAND ----------

# MAGIC %md 
# MAGIC #Random Forest
# MAGIC 
# MAGIC Random forests are ensembles of decision trees. they are one of the strongest machine learning models for classification and regression. They combine many decision trees in order to reduce the risk of overfitting. Like decision trees, random forests:
# MAGIC - Handle categorical features
# MAGIC - Extend to the multiclass classification setting
# MAGIC - Do not require feature scaling
# MAGIC - They are able to capture non-linearities and feature interactions.

# COMMAND ----------

# MAGIC %md ### Initial model

# COMMAND ----------

# MAGIC %md We start with creating an initial RandomForest model and training it on the test data.

# COMMAND ----------

rf = RandomForestClassifier(labelCol="label", featuresCol="features")

rfModel = rf.fit(trainingData)

# COMMAND ----------

# MAGIC %md Next, We make predictions on test data using the `transform()` method.

# COMMAND ----------

predictions = rfModel.transform(testData)

# COMMAND ----------

# MAGIC %md The schema of resulting dataframe is as follow:

# COMMAND ----------

predictions.printSchema()

# COMMAND ----------

# MAGIC %md The model's predictions and probabilities of each prediction class for each observation is accessible:

# COMMAND ----------

predictions.select("label", "prediction", "probability").collect()

# COMMAND ----------

# MAGIC %md We will evaluate our Random Forest model with `BinaryClassificationEvaluator` as well.

# COMMAND ----------

evaluator = BinaryClassificationEvaluator()
evaluator.evaluate(predictions)

# COMMAND ----------

# MAGIC %md ### Tuning the model parameters

# COMMAND ----------

# MAGIC %md Now we can try tuning the model with the `ParamGridBuilder` and the `CrossValidator`.
# MAGIC 
# MAGIC We indicate 3 values for `maxDepth`, 2 values for `maxBin`, and 2 values for `numTrees`, the grid will have 12 parameter settings for `CrossValidator` to choose from. 
# MAGIC 
# MAGIC We create a 3-fold CrossValidator.

# COMMAND ----------

paramGrid = (ParamGridBuilder()
             .addGrid(rf.maxDepth, [2, 4, 6])
             .addGrid(rf.maxBins, [20, 60])
             .addGrid(rf.numTrees, [5, 20])
             .build())

# COMMAND ----------

# MAGIC %md After creating `CrossValidator`, we fit it on the training data:

# COMMAND ----------

# 3-fold CrossValidator
cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)

# It takes a while
cvModel = cv.fit(trainingData)

# COMMAND ----------

# MAGIC %md ### Evaluating the best model

# COMMAND ----------

# MAGIC %md The test set is used to determine accuracy of the best model:

# COMMAND ----------

# Use test set here so we can measure the accuracy of our model on new data
predictions = cvModel.transform(testData)
evaluator.evaluate(predictions)

# COMMAND ----------

# MAGIC %md # Conclusion

# COMMAND ----------

# MAGIC %md Due to class imbalance problem of the data, performance of these models do not provide enough information about their predictive power. The data has 197,082 observations of class 1 or wet condtion and 2,947,048 observations in class 0 or dry condition.
# MAGIC 
# MAGIC Most machine learning algorithms work best when the number of instances of each classes are roughly equal. In our data, the classifiers will tend to classify wet condition as dry condition.
# MAGIC 
# MAGIC To mitigate the class imbalance problem, we can use one of the following approaches: 
# MAGIC - Sampling based approaches 
# MAGIC - Cost function based approaches
# MAGIC 
# MAGIC In the next step of this study, one of the above approaches should be used and only then the performance of models are reliable enough to be compared in order to choose a model for deployment. Further more normalization or scaling in pipline might deem necessary to have access to more machine learning algorithms.  

# COMMAND ----------

-------------------------------------------------------------------- THE END -----------------------------------------------------------------------------------------------------------